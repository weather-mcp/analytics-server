# Production overrides for docker-compose.yml
# Usage: docker-compose -f docker-compose.yml -f docker-compose.prod.yml up -d

version: '3.8'

services:
  # PostgreSQL with TimescaleDB - Production Configuration
  postgres:
    environment:
      # Production credentials should come from .env file
      POSTGRES_DB: ${DB_NAME}
      POSTGRES_USER: ${DB_USER}
      POSTGRES_PASSWORD: ${DB_PASSWORD}
    ports:
      # Don't expose PostgreSQL port in production (only accessible via Docker network)
      - "127.0.0.1:5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      # Production initialization with migrations
      - ./src/database/schema.sql:/docker-entrypoint-initdb.d/01-schema.sql
      - ./src/database/migrations:/docker-entrypoint-initdb.d/migrations
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '1.0'
          memory: 1G
    command:
      - postgres
      - -c
      - shared_buffers=512MB
      - -c
      - effective_cache_size=1536MB
      - -c
      - maintenance_work_mem=256MB
      - -c
      - max_connections=100
      - -c
      - work_mem=16MB
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Redis - Production Configuration
  redis:
    ports:
      # Don't expose Redis port in production
      - "127.0.0.1:6379:6379"
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.5'
          memory: 256M
    command: redis-server --maxmemory 256mb --maxmemory-policy allkeys-lru --save 60 1 --loglevel warning
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "3"

  # API Server - Production Configuration
  api:
    environment:
      NODE_ENV: production
      LOG_LEVEL: ${LOG_LEVEL:-info}
      # Production should use specific CORS origins, not *
      CORS_ORIGIN: ${CORS_ORIGIN:-https://weather-mcp.dev,https://analytics.weather-mcp.dev}
      TRUST_PROXY: "true"
    ports:
      # Only expose internally (Nginx will proxy)
      - "127.0.0.1:3000:3000"
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"

  # Worker Process - Production Configuration
  worker:
    environment:
      NODE_ENV: production
      LOG_LEVEL: ${LOG_LEVEL:-info}
      WORKER_BATCH_SIZE: ${WORKER_BATCH_SIZE:-100}
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"

  # Nginx Reverse Proxy - Production Only
  nginx:
    image: nginx:alpine
    container_name: analytics-nginx
    depends_on:
      - api
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/conf.d:/etc/nginx/conf.d:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
      - nginx_cache:/var/cache/nginx
      - nginx_logs:/var/log/nginx
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 128M
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"

  # Remove dashboard from production (it's in separate website project)
  dashboard:
    profiles:
      - dev
      - test

volumes:
  nginx_cache:
    driver: local
  nginx_logs:
    driver: local

networks:
  default:
    name: analytics-network-prod
    driver: bridge
    ipam:
      config:
        - subnet: 172.28.0.0/16
